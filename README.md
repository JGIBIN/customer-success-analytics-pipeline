<p align="center"> <img alt="Status" src="https://img.shields.io/badge/Status-Em%20Desenvolvimento-orange"> <img alt="Tecnologias" src="https://img.shields.io/badge/Tecnologias-Python%20%7C%20GCP%20%7C%20dbt%20%7C%20SQL-blue"> <img alt="Output" src="https://img.shields.io/badge/Output-BigQuery%20%7C%20Streamlit%20%7C%20Power%20BI-green"> </p>

# Customer Success Analytics Pipeline

**üéØ O Desafio de Neg√≥cio**

Em qualquer empresa de SaaS (como a TOTVS), a reten√ß√£o de clientes √© o principal pilar de crescimento. O grande desafio das equipes de Customer Success (CSM) √© identificar proativamente quais clientes est√£o em risco de churn (cancelamento) para poderem atuar antes que seja tarde demais.

Este projeto constr√≥i o pipeline de dados completo para processar dados de uso, calcular Health Scores e treinar um modelo de predi√ß√£o de churn, transformando dados brutos em a√ß√µes estrat√©gicas de reten√ß√£o.

---

**üèóÔ∏è Arquitetura da Solu√ß√£o**

O projeto seguir√° uma arquitetura ELT (Extract, Load, Transform) moderna, que √© escal√°vel e robusta. Os dados s√£o extra√≠dos em Python, carregados em estado bruto no BigQuery, e s√≥ ent√£o transformados usando dbt e SQL.
Snippet de c√≥digo

    graph TD
        subgraph "Ingest√£o (Python)"
            A[Script Python<br>Gera√ß√£o de Lotes] --> B(GCP BigQuery<br>Dataset: cs_ops_raw_data);
    end

    subgraph "Transforma√ß√£o (dbt)"
        B --> C{dbt Core<br>models/staging/*.sql};
        C --> D(GCP BigQuery<br>Dataset: cs_ops_analytics);
    end

    subgraph "An√°lise & Consumo"
        D --> E[Power BI / Looker<br>Dashboard Executivo];
        D --> F[Streamlit App<br>CS Ops Command Center];
        D --> G[Python / Scikit-learn<br>Modelo de Churn];
    end

---

üíª **Stack de Tecnologias**

    Ingest√£o: Python (Pandas, Faker, pandas-gbq)

    Data Warehouse: Google Cloud Platform (GCP) (BigQuery)

    Transforma√ß√£o: dbt Core & SQL

    An√°lise & Modelagem: Python (Scikit-learn)

    Visualiza√ß√£o: Streamlit & Power BI (ou Looker Studio)

    Ambiente & DevOps: VSCode, Git / GitHub

---

üó∫Ô∏è **Roadmap do Projeto**

Este √© um projeto em desenvolvimento. As etapas abaixo representam o plano de constru√ß√£o.

    [ ] Fase 0: Configura√ß√£o e Infraestrutura

        [X] Criar reposit√≥rio no GitHub (customer-success-analytics-pipeline).

        [ ] Configurar projeto no Google Cloud (GCP).

        [ ] Ativar APIs do BigQuery e Cloud Storage.

        [ ] Criar datasets no BigQuery: cs_ops_raw_data e cs_ops_analytics.

        [ ] Configurar ambiente local (VSCode, Git, Python venv, .gitignore).

        [ ] Configurar autentica√ß√£o (gcloud auth).

    [ ] Fase 1: Ingest√£o de Dados (ELT - Extract & Load)

        [ ] Desenvolver script Python (ingestion/run_ingestion.py).

        [ ] Simular dados "sujos" (nulos, duplicatas, formatos errados).

        [ ] Implementar gera√ß√£o e carga de dados em lotes (batches) para escalar (+1M linhas).

        [ ] Carregar dados no dataset cs_ops_raw_data do BigQuery.

    [ ] Fase 2: Transforma√ß√£o de Dados (ELT - Transform)

        [ ] Iniciar o projeto dbt (cs_ops_dbt/).

        [ ] Configurar sources (fontes) para ler do cs_ops_raw_data.

        [ ] Criar modelos de staging (models/staging/) para limpar, padronizar e deduplicar.

        [ ] Criar modelos de mart (models/marts/) com a tabela anal√≠tica final (fct_clientes_kpis).

        [ ] Executar dbt run e validar os dados limpos no dataset cs_ops_analytics.

    [ ] Fase 3: An√°lise & Modelagem (Machine Learning)

        [ ] Criar script/notebook (analysis/train_model.py).

        [ ] Ler dados limpos da tabela fct_clientes_kpis (do BigQuery).

        [ ] Treinar modelo de classifica√ß√£o (Scikit-learn) para prever churn_status.

        [ ] Salvar o modelo treinado (.pkl).

    [ ] Fase 4: Visualiza√ß√£o & Aplica√ß√£o (BI e Web App)

        [ ] Conectar o Power BI (ou Looker Studio) ao BigQuery (cs_ops_analytics).

        [ ] Criar dashboard executivo (KPIs, MRR, Churn Rate).

        [ ] Desenvolver a aplica√ß√£o "CS Ops Command Center" (app.py) com Streamlit.

        [ ] Implementar no Streamlit um simulador que consome o modelo treinado.

    [ ] Fase 5: Documenta√ß√£o e Finaliza√ß√£o

        [ ] Adicionar screenshots do projeto na se√ß√£o "Vitrine".

        [ ] Preencher a se√ß√£o "Como Executar" com o passo a passo final.

        [ ] (Opcional) Fazer deploy do app Streamlit no Community Cloud.

---

üì∏ **Vitrine do Projeto (Em Breve)**

(Esta se√ß√£o ser√° preenchida com screenshots √† medida que as fases forem conclu√≠das)

    Print 1: Amostra dos dados sujos no BigQuery (raw_data).

    Print 2: Amostra dos dados limpos ap√≥s o dbt run (analytics).

    Print 3: Gr√°fico de Linhagem de Dados (DAG) do dbt docs.

    Print 4: Dashboard final no Power BI / Looker Studio.

    Print 5: Aplica√ß√£o "CS Ops Command Center" rodando no Streamlit.

---

‚ñ∂Ô∏è **Como Executar (Em Breve)**

(Instru√ß√µes detalhadas de instala√ß√£o e execu√ß√£o ser√£o adicionadas na Fase 5.)

    LinkedIn: linkedin.com/in/[seu-linkedin-url]

    GitHub: github.com/[seu-github-url]
